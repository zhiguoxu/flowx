{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from auto_flow.core.llm.openai.openai_llm import OpenAILLM\n",
    "from auto_flow.core.prompts.chat_template import ChatTemplate\n",
    "from auto_flow.core.rag.retrivers.wiki_retriever import WikiRetriever\n",
    "\n",
    "llm = OpenAILLM(model=\"gpt-4o\", temperature=0)\n",
    "wiki = WikiRetriever(top_k=6, doc_content_chars_max=2000)\n",
    "prompt = ChatTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, answer the user question. If none of the articles answer the question, just say you don't know.\\n\\nHere are the Wikipedia articles:{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.rag.document.document import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Convert Documents to a single string.:\"\"\"\n",
    "    formatted = [\n",
    "        f\"Article Title: {doc.metadata['title']}\\nArticle Snippet: {doc.text}\"\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.llm.message_parser import StrOutParser\n",
    "from auto_flow.core.flow.flow import to_flow, ParallelFlow, identity\n",
    "from operator import itemgetter\n",
    "\n",
    "context = itemgetter(\"docs\") | to_flow(format_docs)\n",
    "answer = prompt | llm | StrOutParser()\n",
    "flow = (\n",
    "    ParallelFlow(input=identity, docs=wiki)\n",
    "    .assign(context=context)\n",
    "    .assign(answer=answer)\n",
    "    .pick(\"answer\", \"docs\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flow.invoke(\"How fast are cheetahs?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function-calling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class cited_answer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        description=\"The answer to the user question, which is based only on the given sources.\"\n",
    "    )\n",
    "    citations: List[int] = Field(\n",
    "        description=\"The integer IDs of the SPECIFIC sources which justify the answer.\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_with_tool = llm.bind_tools([cited_answer], tool_choice=\"cited_answer\")\n",
    "example_q = \"\"\"What Brian's height?\n",
    "\n",
    "Source: 1\n",
    "Information: Suzy is 6'2\"\n",
    "\n",
    "Source: 2\n",
    "Information: Jeremiah is blonde\n",
    "\n",
    "Source: 3\n",
    "Information: Brian is 3 inches shorted than Suzy\"\"\"\n",
    "llm_with_tool.invoke(example_q)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.llm.message_parser import MessagePydanticOutParser\n",
    "\n",
    "output_parser = MessagePydanticOutParser(return_dict=True, return_first=True)\n",
    "(llm_with_tool | output_parser).invoke(example_q)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.rag.document.document import Document\n",
    "from auto_flow.core.flow.flow import to_flow, ParallelFlow, identity\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def format_docs_with_id(docs: List[Document]) -> str:\n",
    "    formatted = [\n",
    "        f\"Source ID: {i}\\nArticle Title: {doc.metadata['title']}\\nArticle Snippet: {doc.text}\"\n",
    "        for i, doc in enumerate(docs)\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "format_1 = itemgetter(\"docs\") | to_flow(format_docs_with_id)\n",
    "answer_1 = prompt | llm_with_tool | output_parser\n",
    "chain_1 = (\n",
    "    ParallelFlow(input=identity, docs=wiki)\n",
    "    .assign(context=format_1)\n",
    "    .assign(cited_answer=answer_1)\n",
    "    .pick(\"cited_answer\", \"docs\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chain_1.invoke(\"How fast are cheetahs?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cite snippets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Citation(BaseModel):\n",
    "    source_id: int = Field(\n",
    "        description=\"The integer ID of a SPECIFIC source which justifies the answer.\",\n",
    "    )\n",
    "    quote: str = Field(\n",
    "        description=\"The VERBATIM quote from the specified source that justifies the answer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class quoted_answer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        description=\"The answer to the user question, which is based only on the given sources.\"\n",
    "    )\n",
    "    citations: List[Citation] = Field(\n",
    "        description=\"Citations from the given sources that justify the answer.\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_parser_2 = MessagePydanticOutParser(return_dict=True, return_first=True)\n",
    "llm_with_tool_2 = llm.bind_tools([quoted_answer], tool_choice=\"quoted_answer\")\n",
    "format_2 = itemgetter(\"docs\") | to_flow(format_docs_with_id)\n",
    "answer_2 = prompt | llm_with_tool_2 | output_parser_2\n",
    "chain_2 = (\n",
    "    ParallelFlow(input=identity, docs=wiki)\n",
    "    .assign(context=format_2)\n",
    "    .assign(quoted_answer=answer_2)\n",
    "    .pick(\"quoted_answer\", \"docs\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chain_2.invoke(\"How fast are cheetahs?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
