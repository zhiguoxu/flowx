{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Chain without chat history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.llm.openai.openai_llm import OpenAILLM\n",
    "\n",
    "llm = OpenAILLM(model=\"gpt-4o\", temperature=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.rag.splitter.text_splitter import TextSplitter\n",
    "from auto_flow.core.rag.vectorstore.chroma import Chroma\n",
    "from auto_flow.core.rag.document_loaders.dir_loader import DirLoader\n",
    "\n",
    "docs = DirLoader(file_or_dir=\"./files/paul_graham_essay.txt\").load()\n",
    "docs = TextSplitter().split_document(docs)\n",
    "vectorstore = Chroma()\n",
    "vectorstore.add_documents(docs)\n",
    "retriever = vectorstore.as_retriever(top_k=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.rag.document.document import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def format_docs(documents: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.text for doc in documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.llm.message_parser import StrOutParser\n",
    "from auto_flow.core.rag.prompts import qa_prompt\n",
    "from auto_flow.core.flow.flow import identity\n",
    "\n",
    "rag_flow = (\n",
    "        {\"context\": retriever | format_docs, \"input\": identity}\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rag_flow.invoke(\"what did paul graham do after going to RISD?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement with helpers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.rag.helpers import create_qa_flow\n",
    "\n",
    "rag_flow2 = create_qa_flow(retriever, llm)\n",
    "rag_flow2.invoke(\"what did paul graham do after going to RISD?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Contextualizing the question"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.rag.retrivers.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chain with chat history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from auto_flow.core.rag.helpers import create_rag_flow\n",
    "from auto_flow.core.rag.combine_documents.stuff import create_stuff_documents_flow\n",
    "from auto_flow.core.rag.prompts import qa_prompt\n",
    "\n",
    "question_answer_flow = create_stuff_documents_flow(qa_prompt, llm)\n",
    "rag_flow = create_rag_flow(history_aware_retriever, question_answer_flow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.messages.chat_message import ChatMessage, Role\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"what did paul graham do after going to RISD?\"\n",
    "ai_msg_1 = rag_flow.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([ChatMessage(role=Role.USER, content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"Where dit he work after that?\"\n",
    "ai_msg_2 = rag_flow.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement with helpers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from auto_flow.core.rag.helpers import create_qa_flow_with_history\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "rag_flow2 = create_qa_flow_with_history(retriever, llm)\n",
    "question = \"what did paul graham do after going to RISD?\"\n",
    "ai_msg_1 = rag_flow2.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([ChatMessage(role=Role.USER, content=question), ai_msg_1])\n",
    "\n",
    "second_question = \"Where dit he work after that?\"\n",
    "ai_msg_2 = rag_flow2.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
